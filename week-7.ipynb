{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Week 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!/usr/bin/python\n\n# Try to open imports\ntry:\n    import sys\n    import random\n    import math\n    import os\n    import time\n    import numpy as np\n    import pandas as pd\n    from random import sample \n    from matplotlib import pyplot as plt\n    import matplotlib.colorbar\n    from sklearn import datasets\n    from sklearn.model_selection import StratifiedShuffleSplit\n    from sklearn.model_selection import train_test_split\n    from sklearn.neighbors import NearestNeighbors\n    from sklearn.neighbors import KDTree\n    from sklearn.datasets import load_iris\n    from sklearn.model_selection import train_test_split\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.metrics import confusion_matrix\n    from sklearn.cross_validation import train_test_split\n    from sklearn.decompostion import PCA as sklearnPCA\n    from sklearn.model_selection import cross_val_score\n    from scipy import stats\n    from sklearn.metrics import mean_squared_error\n    from sklearn.cluster import KMeans\n    from sklearn.datasets import make_blobs\n    from sklearn.decompostion import PCA as sklearnPCA\n    from scipy.spatial import distance\n    from sklearn.model_selection import train_test_split\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.model_selection import GridSearchCV\n    from sklearn.metrics import classification_report\n    from dtaidistance import dtw\n    from dtaidistance import dtw_visualisation as dtwvis\n\n# Error when importing\nexcept ImportError:\n    print('### ', ImportError, ' ###')\n    # Exit program\n    exit()\n\n# DTW\ndef dtw(s, t):\n    n, m = len(s), len(t)\n    dtw_matrix = np.zeros((n+1, m+1))\n    for i in range(n+1):\n        for j in range(m+1):\n            dtw_matrix[i, j] = np.inf\n    dtw_matrix[0, 0] = 0\n    \n    for i in range(1, n+1):\n        for j in range(1, m+1):\n            cost = abs(s[i-1] - t[j-1])\n            # take last min from a square box\n            last_min = np.min([dtw_matrix[i-1, j], dtw_matrix[i, j-1], dtw_matrix[i-1, j-1]])\n            dtw_matrix[i, j] = cost + last_min\n    return dtw_matrix\n\n#tokenize\ndef tokenize(s):\n    return tokens_re.findall(s)\n\n#Cosine Sim\ndef cosine_sim(u, v):\n    return dot(u, v) / (norm(v) * norm(u))\n\n#Stemmed Words\ndef stemmed_words(doc):\n    return (stemmer.stem(w) for w in analyzer(doc))\n\n#Average word vectors\ndef average_word_vectors(words, model, vocabulary, num_features):\n    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n    nwords = 0\n    for word in words:\n        if word in vocabulary:\n            nwords = nwords + 1\n            feature_vector = np.add(feature_vector, model[word])\n    if nwords:\n        feature_vector = np.divide(feature_vector, nwords)\n    retrun feature_vector\n\n\ndef average_word_vectorizer(corpus, model, num_features):\n    vocabulary = set(model.wv.index2word)\n    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features) for tokenized_sentence in corpus]\n    retrun np.array(features)\n\n\n# Main\ndef main():\n    vocabulary = ['King', 'Man','Queen','Woman']\n    tokens = {w:i for i, w in enumerate(vocabulary)}\n    N = len(vocabulary)\n    w = np.zeros((N, N))\n    np.fill_diagonal(w, 1)\n    print(\"Cosine Similarity \".format(cosine_sim(w['King'], w['woman'])))\n    Q = \"tesla nasa\"\n    tfidf = TfidfVectorizer(analyer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english', max_features=500)\n    features = tfidf.fit(original_documents)\n    cporus_tf_idf = tfidf.transform(original_documents)\n    sum_words = corpus_tf_idf.sim(axis = 0)\n    words_freq = [(word, sum_words[0, indx]) for word, idx in tfidf.vocabulary_.items()]\n    print(sorted(wrods_freq, key = lambda x: x[1], reverse = true)[:5])\n    print('test', corpus_tf_idf[1], features.vocabulary_['tesla'])\n    new_features = tfidf.transform([querycosine_similarities = linear_kernel(new_features, corpus_tf_idf).flatten()\n    related_docs_indices = cosine_similarities.argsort()[::-1]\n    topk = 5\n    print('Top - {0} documents'.format(topk))\n    for i in range(topk):\n        print(i, original_documents[related_docs_indices[i]]) \n    stemmer = FrenchStemmer()\n    analyzer = CountVectorizer().build_analyzer()\n    stem_vectorizer = CountVectorizer(analyzer=stemmed_words)\n    print(stem_vectorizer.fit_transform(['Tu marches dans la rue']))\n    print(stem_vectorizer.get_feature_names())\n    tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n    print(word_tokenize(tweet))\n    with open('mytweets.json', 'r') as f:\n    line = f.readline() \n    tweet = json.loads(line) \n    print(json.dumps(tweet, indent=4))\n    with open('mytweets.json', 'r') as f:\n    for line in f:\n        tweet = json.loads(line)\n        tokens = preprocess(tweet['text'])\n        do_something_else(tokens)  \n    # Sentiment Analysis\n    n_instances = 100\n    subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n    obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n    sd = len(subj_docs)\n    od = len(obj_docs)\n    subj_docs[0]\n    train_subj_docs = subj_docs[:80]\n    test_subj_docs = subj_docs[80:100]\n    train_obj_docs = obj_docs[:80]\n    test_obj_docs = obj_docs[80:100]\n    training_docs = train_subj_docs + train_obj_docs\n    testing_docs = test_subj_docs + test_obj_docs\n    sentim_analyzer = SentimentAnalyzer()\n    all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n    unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n    uf = len(unigram_feats)\n    # Classification\n    sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n    training_set = sentim_analyzer.apply_features(training_docs)\n    test_set = sentim_analyzer.apply_features(testing_docs)\n    trainer = NaiveBayesClassifier.train\n    classifier = sentim_analyzer.train(trainer, training_set)\n    for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n        print('{0}: {1}'.format(key, value))\n    # Vader\n    words = ['ice', 'police', 'global']\n    document = 'ice is melting due to global warming'.split()\n    sorted(extract_unigram_feats(document, words).items())\n    sentences = [\"VADER is smart, handsome, and funny.\", \"VADER is smart, handsome, and funny!\"]\n    lines_list = tokenize.sent_tokenize(paragraph)\n    sentences.extend(lines_list)\n    tricky_sentences = [\"The movie was too good\", \"This movie was actually neither that funny, nor super witty.\"]\n    sentences.extend(tricky_sentences)\n    sid = SentimentIntensityAnalyzer()\n    for sentence in sentences:\n        ss = sid.polarity_scores(sentence)\n        for k in sorted(ss):\n             print('{0}: {1}, '.format(k, ss[k]), end='')\n        print()\n                                    \n\n    # Close Program\n    exit()\n   \n\n# init\nif __name__ == '__main__':\n    # Begin\n    main()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}